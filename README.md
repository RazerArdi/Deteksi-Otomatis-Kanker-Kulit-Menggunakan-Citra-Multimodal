
<div align="center">

# ğŸ”¬ MILKFusionNet
### *Comparative Framework: Classic ML vs. CNNs vs. Transformers with PEFT/LoRA*

![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge&logo=github)
![Framework](https://img.shields.io/badge/Framework-PyTorch_%7C_Scikit--Learn-EE4C2C?style=for-the-badge&logo=pytorch)
![Technique](https://img.shields.io/badge/Technique-LoRA_%7C_Imbalance_Handling-violet?style=for-the-badge&logo=huggingface)
![Dataset](https://img.shields.io/badge/Dataset-ISIC%20MILK--10k-00D4FF?style=for-the-badge&logo=kaggle)
![Language](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

**[Dataset](https://challenge.isic-archive.com/data/#milk10k)** â€¢ **[Methodology](#-methodology)** â€¢ **[Results](#-results--benchmarking)** â€¢ **[License](#-license)**

</div>

---

## ğŸ¯ Overview

**MILKFusionNet** is a comprehensive research framework designed to benchmark skin lesion classification strategies on the **ISIC MILK-10k** dataset (11 Diagnostic Categories). Moving beyond simple binary classification, this project rigorously evaluates **22 model variations** across three distinct paradigms:

1.  **Classic Machine Learning:** Interpretable baselines using hand-crafted feature engineering (Color, Texture, Shape).
2.  **Deep Learning (CNNs):** Benchmarking Custom architectures vs. Industry-standard backbones (ResNet, VGG, EfficientNet).
3.  **Vision Transformers:** Evaluating State-of-the-Art attention mechanisms (ViT, Swin, DeiT).

> **Core Innovation:** The application of **Low-Rank Adaptation (LoRA)** on both CNN and Transformer architectures to demonstrate that **>99% parameter efficiency** can be achieved without compromising diagnostic accuracy on imbalanced medical datasets.

---

## ğŸ—ï¸ Architecture & Workflows

The project executes three parallel experimental workflows:

```mermaid
graph TD;
    subgraph "Workflow A: Classic ML (Baseline)"
        A1[Input Image] --> B1[Feature Extraction];
        B1 --> C1["Features: RGB, HSV, GLCM, HOG, LBP"];
        C1 --> D1["Imbalance: SMOTE / Class Weights"];
        D1 --> E1["Models: SVM, Random Forest, XGBoost"];
    end

    subgraph "Workflow B: CNNs (Deep Learning)"
        A2["Input Image (224x224)"] --> B2["Augmentation + Normalization"];
        B2 --> C2["Architectures: CustomCNN, ResNet50, VGG16, EffNetB0"];
        C2 --> D2["Strategies: Scratch, Frozen, Full-FT, LoRA"];
        D2 --> E2["Training (Weighted Loss)"];
    end

    subgraph "Workflow C: Vision Transformers"
        A3["Input Image (224x224)"] --> B3["Augmentation + Normalization"];
        B3 --> C3["Architectures: ViT-Base, Swin-Tiny, DeiT-Base"];
        C3 --> D3["Strategies: Scratch, Full-FT, LoRA"];
        D3 --> E3["Training (Weighted Loss)"];
    end

    E1 & E2 & E3 --> F["ğŸ† Grand Final Analysis"];
    F --> G["Efficiency Plots (Acc vs Params) & Dual-Lang Posters"];
````

-----

## ğŸ› ï¸ Methodology

### 1\. Dataset & Preprocessing ğŸ–¼ï¸

  * **Dataset:** ISIC Archive MILK-10k (11 Classes: Melanoma, Nevus, BCC, etc.).
  * **Preprocessing:** Resize (224x224), Normalization (ImageNet Mean/Std).
  * **Augmentation:** Random Horizontal/Vertical Flip, Rotation (20Â°), Color Jitter.
  * **Imbalance Handling:**
      * *Classic ML:* SMOTE Oversampling.
      * *Deep Learning:* **Weighted Cross-Entropy Loss** (Calculated based on inverse class frequency).

### 2\. Model Configurations ğŸ¤–

We evaluated a total of **22 model configurations** across three distinct paradigms:

| Category | Architecture | Training Strategies | Technical Details |
|:---|:---|:---|:---|
| **Classic ML** | **SVM, Random Forest, XGBoost** | Feature Engineering | Inputs: GLCM, HOG, Color Moments (30 Features) |
| **CNN** | **CustomVanillaCNN** | Scratch | Lightweight Baseline (3 Conv Blocks) |
| **CNN** | **ResNet50** | Frozen, Full FT, **LoRA** | LoRA Target: `layerX.conv2` |
| **CNN** | **VGG16** | Frozen, Full FT, **LoRA** | LoRA Target: `features` blocks |
| **CNN** | **EfficientNet-B0** | Frozen, Full FT, **LoRA** | LoRA Target: `MBConv` expansion layers |
| **Transformer** | **ViT-Base** | Scratch, Full FT, **LoRA** | LoRA Target: `query`, `value` |
| **Transformer** | **Swin-Tiny** | Scratch, Full FT, **LoRA** | Hierarchical Window Attention |
| **Transformer** | **DeiT-Base** | Scratch, Full FT, **LoRA** | Distilled Knowledge (Replaces MaxViT) |

### 3. Experimental Setup & Hyperparameters âš™ï¸

To ensure reproducibility, all 22 models were trained using a unified hyperparameter configuration. We established strict "apple-to-apple" comparison protocols between CNNs and Transformers.

#### A. Global Configurations
| Parameter | Value | Description |
| :--- | :--- | :--- |
| **Image Size** | `224 x 224` | Standard ImageNet resolution |
| **Batch Size** | `32` | Optimized for 16GB VRAM GPU |
| **Epochs** | `10` | Early stopping monitored on Validation Loss |
| **Loss Function** | `Weighted CrossEntropy` | Weights calculated via inverse class frequency |
| **Num Workers** | `0` | Set for Windows/Jupyter environment stability |
| **Seed** | `42` | Fixed for reproducibility |

#### B. Architecture-Specific Settings
We utilized distinct optimizers and learning rates tailored to the inductive bias of each architecture family:

| Hyperparameter | CNNs (ResNet/VGG/EffNet) | Transformers (ViT/Swin/DeiT) |
| :--- | :--- | :--- |
| **Optimizer** | `Adam` | `AdamW` (Crucial for attention mechanisms) |
| **Learning Rate** | `1e-4` (0.0001) | `2e-5` (0.00002) - *Lower for stability* |
| **Weight Decay** | `1e-2` | `1e-2` |
| **LR Scheduler** | ReduceLROnPlateau | Cosine Annealing |

#### C. LoRA (PEFT) Configuration
For models using Low-Rank Adaptation, we froze the backbone and injected trainable rank decomposition matrices with the following specs:

| Parameter | Setting | Impact |
| :--- | :--- | :--- |
| **Rank (r)** | `16` | The "sweet spot" balancing efficiency and capacity |
| **Alpha** | `32` | Scaling factor (typically 2x Rank) |
| **Dropout** | `0.1` | Regularization for adapters |
| **Bias** | `None` | Biases are not trained to save memory |
| **Target Modules** | **CNN:** `conv` layers<br>**ViT/DeiT:** `query`, `value`<br>**Swin:** `q`, `v` (Attention) | Specific injection points per architecture |

#### D. Classic ML Setup (Stream 1)
* **Feature Extraction:** 30 handcrafted features (12 RGB + 6 HSV + 6 GLCM + 3 HOG + 3 LBP).
* **Imbalance Handling:** SMOTE (Synthetic Minority Over-sampling Technique).
* **Classifiers:** SVM (RBF Kernel), Random Forest (n=100), XGBoost.

-----

## ğŸ“Š Results & Benchmarking

The pipeline automatically generates **High-DPI** (300 DPI) visualizations stored in `notebook/image/`, featuring dual-language support (English & Indonesian).

### 1\. Efficiency Analysis (The "PhD" Insight)

We performed a trade-off analysis between Accuracy and Model Size:

  * **Scatter Plot:** Visualizes models in the "High Accuracy - Low Params" quadrant.
  * **Bar Chart (Log Scale):** Demonstrates the extreme parameter reduction achieved by LoRA (from \~86 Million to \~300 Thousand trainable parameters).

### 2\. Key Visualizations

  * **Training Dynamics:** Loss/Accuracy curves per epoch comparing DL models against the Classic ML baseline.
  * **Confusion Matrix:** Heatmaps identifying specific inter-class confusion (e.g., differentiating *Melanoma* from *Nevus*).
  * **Leaderboard:** Horizontal bar charts ranking all 22 models based on Validation Accuracy.

-----

## ğŸ“‚ Project Structure

The repository is structured as follows:

```bash
MILKFusionNet/
â”œâ”€â”€ ğŸ“‚ dataset/                  # Raw ISIC data (Train/Test splits)
â”‚
â”œâ”€â”€ ğŸ“‚ Models/                   # ğŸ’¾ Saved Model Weights (.pth)
â”‚   â”œâ”€â”€ CustomCNN_Scratch.pth
â”‚   â”œâ”€â”€ ResNet50_LoRA.pth
â”‚   â”œâ”€â”€ DeiT_LoRA.pth
â”‚   â””â”€â”€ ... (All 22 models)
â”‚
â”œâ”€â”€ ğŸ“‚ notebook/                 # ğŸ““ Main Experiment Environment
â”‚   â”œâ”€â”€ main.ipynb               # End-to-End Pipeline Code
â”‚   â”œâ”€â”€ poster_table_results.csv # Raw results data
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ image/                # ğŸ–¼ï¸ GENERATED OUTPUTS (High-Res)
â”‚       â”œâ”€â”€ all_model_results.csv
â”‚       â”œâ”€â”€ Fig1_Data_Distribution_English.png
â”‚       â”œâ”€â”€ Fig2_Training_Dynamics_English.png
â”‚       â”œâ”€â”€ Fig5_Parameter_Efficiency_English.png
â”‚       â”œâ”€â”€ Fig6_Accuracy_vs_Params_English.png
â”‚       â””â”€â”€ ... (Confusion Matrices for every model)
â”‚
â”œâ”€â”€ ğŸ“‚ processed_data/           # Intermediate files (Efficiency stats)
â”œâ”€â”€ ğŸ“„ requirements.txt          # Project dependencies
â””â”€â”€ ğŸ“„ README.md                 # This documentation
```

-----

## ğŸš€ Getting Started

### Prerequisites

Ensure your environment supports CUDA (GPU recommended for Transformers).

```bash
# Install core libraries
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu118](https://download.pytorch.org/whl/cu118)

# Install specific research tools
pip install transformers peft scikit-learn pandas matplotlib seaborn opencv-python tqdm imbalanced-learn
```

### Execution Guide

Run the `notebook/main.ipynb` cells sequentially:

1.  **Data Prep:** Load dataset, perform stratified splitting, and apply augmentation.
2.  **Workflow A:** Feature extraction & Classic ML training.
3.  **Workflow B:** CNN Factory setup & Training Loop (10 Models).
4.  **Workflow C:** Transformer Factory setup & Training Loop (9 Models).
5.  **Grand Final:** Execute the final visualization cells to generate posters and CSVs.

-----

## ğŸ‘¨â€ğŸ”¬ Author

**Bayu Ardiyansyah**

  * **Focus:** Medical Image Analysis, Deep Learning, Computer Vision.
  * **Tech Stack:** PyTorch, Scikit-Learn, Hugging Face.

-----

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](https://www.google.com/search?q=LICENSE) file for details.

-----

<div align="center">

> **Disclaimer:** MILKFusionNet is a research prototype. Predictions should not replace professional medical diagnosis.

**â­ Star this repository if you find it useful for your research\!**

</div>